{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:06:34.980012965Z",
     "start_time": "2024-02-22T19:06:32.040761466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: torch in /home/abhilash/.local/lib/python3.10/site-packages (2.2.0)\r\n",
      "Requirement already satisfied: torchtext in /home/abhilash/.local/lib/python3.10/site-packages (0.17.0)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.1)\r\n",
      "Requirement already satisfied: sentencepiece in /home/abhilash/.local/lib/python3.10/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\r\n",
      "Requirement already satisfied: datasets in /home/abhilash/.local/lib/python3.10/site-packages (2.17.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abhilash/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.3)\r\n",
      "Requirement already satisfied: torchdata==0.7.1 in /home/abhilash/.local/lib/python3.10/site-packages (from torchtext) (0.7.1)\r\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/lib/python3/dist-packages (from torchdata==0.7.1->torchtext) (1.26.5)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (15.0.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: xxhash in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (3.9.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext) (2020.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abhilash/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abhilash/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3.10 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchtext transformers sentencepiece pandas tqdm datasets"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:06:36.146612628Z",
     "start_time": "2024-02-22T19:06:34.986160436Z"
    }
   },
   "id": "34702c45c363d328",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load data\n",
    "data_sample = load_dataset(\"csv\", data_files=\"merged_file.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:06:37.493293170Z",
     "start_time": "2024-02-22T19:06:36.151833714Z"
    }
   },
   "id": "9f304f37f66bed64",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'cell_id_x', 'notebook_id', 'code_imports', 'defined_functions', 'source_x', 'cell_id_y', 'source_y'],\n        num_rows: 828122\n    })\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:06:37.570386068Z",
     "start_time": "2024-02-22T19:06:37.508084602Z"
    }
   },
   "id": "a34b53a14d41b48c",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "updated_data = [{'Code': item['source_x'], 'Description': item['source_y']} for item in data_sample['train']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:07:07.372078408Z",
     "start_time": "2024-02-22T19:06:37.526233464Z"
    }
   },
   "id": "289b592c37c45d5d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.DataFrame(updated_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:13:49.461494946Z",
     "start_time": "2024-02-22T19:13:49.172717662Z"
    }
   },
   "id": "cc4929fb267d4965",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 Code  \\\n0   import os\\nPROJECT = \"cloud-training-demos\" # ...   \n1   %%bash\\ngcloud config set project $PROJECT\\ngc...   \n2   %%bash\\nrm -rf mnistmodel.tar.gz mnist_trained...   \n3   %%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...   \n4   from google.datalab.ml import TensorBoard\\nTen...   \n5   for pid in TensorBoard.list()[\"pid\"]:\\n    Ten...   \n6   %%bash\\nMODEL_NAME=\"mnist\"\\nMODEL_VERSION=${MO...   \n7   import json, codecs\\nimport matplotlib.pyplot ...   \n8   %%bash\\ngcloud ml-engine predict \\\\n    --mode...   \n9   trainingInput:\\n    scaleTier: CUSTOM\\n    mas...   \n10  %%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...   \n11  from subprocess import check_output\\nprint(che...   \n12            import numpy as np\\nimport pandas as pd   \n13                                   embedding_matrix   \n14  from keras.models import Model\\nfrom keras.lay...   \n15  EMBEDDING_FILE = f'glove.6B.50d.txt'\\nTRAIN_DA...   \n16  max_features = 20000\\nmaxlen = 100\\nembed_size...   \n17               train = pd.read_csv(TRAIN_DATA_FILE)   \n18                                       train.head()   \n19  #train = train.sample(frac=1)\\ntest = pd.read_...   \n20                                        test.head()   \n21  list_sentences_train = train['comment_text'].f...   \n22                       #print(list_sentences_train)   \n23  list_classes = [\"toxic\", \"severe_toxic\", \"obsc...   \n24                                       list_classes   \n25                     y = train[list_classes].values   \n26                                                  y   \n27                                            y.shape   \n28  list_sentences_test = test['comment_text'].fil...   \n29                        #print(list_sentences_test)   \n30  tokenizer = text.Tokenizer(num_words=max_featu...   \n31                                   print(tokenizer)   \n32  tokenizer.fit_on_texts(list(list_sentences_tra...   \n33  list_tokenized_train = tokenizer.texts_to_sequ...   \n34                        print(list_tokenized_train)   \n35  X_t = sequence.pad_sequences(list_tokenized_tr...   \n36                                          X_t.shape   \n37                                         X_te.shape   \n38  def get_coefs(word,*arr): \\n    return word, n...   \n39     all_embs = np.stack(embeddings_index.values())   \n40  emb_mean,emb_std = all_embs.mean(), all_embs.s...   \n41                                   emb_mean,emb_std   \n42                  word_index = tokenizer.word_index   \n43                                  print(word_index)   \n44      nb_words = min(max_features, len(word_index))   \n45                                           nb_words   \n46  embedding_matrix = np.random.normal(emb_mean, ...   \n47                                   embedding_matrix   \n48  for word, i in word_index.items():\\n    if i >...   \n49  def get_model():\\n    inp = Input(shape=(maxle...   \n\n                                          Description  \n0   # MNIST Image Classification with TensorFlow o...  \n1   # MNIST Image Classification with TensorFlow o...  \n2   ## Run as a Python module\\n\\nIn the previous n...  \n3   **Now, let's do it on Cloud ML Engine so we ca...  \n4   ## Monitoring training with TensorBoard\\n\\nUse...  \n5   ## Monitoring training with TensorBoard\\n\\nUse...  \n6   ## Deploying and predicting with model\\n\\nDepl...  \n7   To predict with the model, let's take one of t...  \n8                   Send it to the prediction service  \n9   ## DO NOT RUN anything beyond this point\\n\\nTh...  \n10  This takes <b>13 hours and 250 ML Units</b>, s...  \n11                      # Keras - Bidirectional LSTM   \n12                      # Keras - Bidirectional LSTM   \n13       ### Read Glove Word Vectors : word -> vector  \n14                              ### Import keras Libs  \n15                              ### Import keras Libs  \n16                              ### Import keras Libs  \n17                              ### Import keras Libs  \n18                              ### Import keras Libs  \n19                              ### Import keras Libs  \n20                              ### Import keras Libs  \n21                              ### Import keras Libs  \n22                              ### Import keras Libs  \n23                              ### Import keras Libs  \n24                              ### Import keras Libs  \n25                              ### Import keras Libs  \n26                              ### Import keras Libs  \n27                              ### Import keras Libs  \n28                              ### Import keras Libs  \n29                              ### Import keras Libs  \n30                              ### Import keras Libs  \n31                              ### Import keras Libs  \n32                              ### Import keras Libs  \n33                              ### Import keras Libs  \n34                              ### Import keras Libs  \n35                              ### Import keras Libs  \n36                              ### Import keras Libs  \n37                              ### Import keras Libs  \n38       ### Read Glove Word Vectors : word -> vector  \n39       ### Read Glove Word Vectors : word -> vector  \n40       ### Read Glove Word Vectors : word -> vector  \n41       ### Read Glove Word Vectors : word -> vector  \n42       ### Read Glove Word Vectors : word -> vector  \n43       ### Read Glove Word Vectors : word -> vector  \n44       ### Read Glove Word Vectors : word -> vector  \n45       ### Read Glove Word Vectors : word -> vector  \n46       ### Read Glove Word Vectors : word -> vector  \n47       ### Read Glove Word Vectors : word -> vector  \n48       ### Read Glove Word Vectors : word -> vector  \n49       ### Read Glove Word Vectors : word -> vector  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Code</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>import os\\nPROJECT = \"cloud-training-demos\" # ...</td>\n      <td># MNIST Image Classification with TensorFlow o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>%%bash\\ngcloud config set project $PROJECT\\ngc...</td>\n      <td># MNIST Image Classification with TensorFlow o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>%%bash\\nrm -rf mnistmodel.tar.gz mnist_trained...</td>\n      <td>## Run as a Python module\\n\\nIn the previous n...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>%%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...</td>\n      <td>**Now, let's do it on Cloud ML Engine so we ca...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>from google.datalab.ml import TensorBoard\\nTen...</td>\n      <td>## Monitoring training with TensorBoard\\n\\nUse...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>for pid in TensorBoard.list()[\"pid\"]:\\n    Ten...</td>\n      <td>## Monitoring training with TensorBoard\\n\\nUse...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>%%bash\\nMODEL_NAME=\"mnist\"\\nMODEL_VERSION=${MO...</td>\n      <td>## Deploying and predicting with model\\n\\nDepl...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>import json, codecs\\nimport matplotlib.pyplot ...</td>\n      <td>To predict with the model, let's take one of t...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>%%bash\\ngcloud ml-engine predict \\\\n    --mode...</td>\n      <td>Send it to the prediction service</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>trainingInput:\\n    scaleTier: CUSTOM\\n    mas...</td>\n      <td>## DO NOT RUN anything beyond this point\\n\\nTh...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>%%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...</td>\n      <td>This takes &lt;b&gt;13 hours and 250 ML Units&lt;/b&gt;, s...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>from subprocess import check_output\\nprint(che...</td>\n      <td># Keras - Bidirectional LSTM</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>import numpy as np\\nimport pandas as pd</td>\n      <td># Keras - Bidirectional LSTM</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>embedding_matrix</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>from keras.models import Model\\nfrom keras.lay...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>EMBEDDING_FILE = f'glove.6B.50d.txt'\\nTRAIN_DA...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>max_features = 20000\\nmaxlen = 100\\nembed_size...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>train = pd.read_csv(TRAIN_DATA_FILE)</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>train.head()</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>#train = train.sample(frac=1)\\ntest = pd.read_...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>test.head()</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>list_sentences_train = train['comment_text'].f...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>#print(list_sentences_train)</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>list_classes = [\"toxic\", \"severe_toxic\", \"obsc...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>list_classes</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>y = train[list_classes].values</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>y</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>y.shape</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>list_sentences_test = test['comment_text'].fil...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>#print(list_sentences_test)</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>tokenizer = text.Tokenizer(num_words=max_featu...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>print(tokenizer)</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>tokenizer.fit_on_texts(list(list_sentences_tra...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>list_tokenized_train = tokenizer.texts_to_sequ...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>print(list_tokenized_train)</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>X_t = sequence.pad_sequences(list_tokenized_tr...</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>X_t.shape</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>X_te.shape</td>\n      <td>### Import keras Libs</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>def get_coefs(word,*arr): \\n    return word, n...</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>all_embs = np.stack(embeddings_index.values())</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>emb_mean,emb_std = all_embs.mean(), all_embs.s...</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>emb_mean,emb_std</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>word_index = tokenizer.word_index</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>print(word_index)</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>nb_words = min(max_features, len(word_index))</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>nb_words</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>embedding_matrix = np.random.normal(emb_mean, ...</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>embedding_matrix</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>for word, i in word_index.items():\\n    if i &gt;...</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>def get_model():\\n    inp = Input(shape=(maxle...</td>\n      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:13:50.320442861Z",
     "start_time": "2024-02-22T19:13:50.280835514Z"
    }
   },
   "id": "4886ab76dc27ef30",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:13:55.507745522Z",
     "start_time": "2024-02-22T19:13:55.497939820Z"
    }
   },
   "id": "a3a3ea3ec52da81f",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        device = torch.device('mps')\n",
    "    except Exception:\n",
    "        device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:13:58.180461726Z",
     "start_time": "2024-02-22T19:13:58.172060274Z"
    }
   },
   "id": "6ecaf6999b2ee235",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:13:59.527451648Z",
     "start_time": "2024-02-22T19:13:59.513064714Z"
    }
   },
   "id": "e5e9f9e5ee08071b",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"sagard21/python-code-explainer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:14:04.076657287Z",
     "start_time": "2024-02-22T19:14:01.873976402Z"
    }
   },
   "id": "6bf0fdfb52d66246",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "T5ForConditionalGeneration(\n  (shared): Embedding(32100, 1024)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32100, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32100, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1024, out_features=32100, bias=False)\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:14:05.049893989Z",
     "start_time": "2024-02-22T19:14:05.029814784Z"
    }
   },
   "id": "5e686b7be01c3d56",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:14:06.181958909Z",
     "start_time": "2024-02-22T19:14:06.168803945Z"
    }
   },
   "id": "a178ff7ec8382d4",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             Code                        Description\ncount      789629                             826331\nunique     552574                             237424\ntop     df.head()  # Vertical Scan with Space Charge\nfreq         1028                               1015",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Code</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>789629</td>\n      <td>826331</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>552574</td>\n      <td>237424</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>df.head()</td>\n      <td># Vertical Scan with Space Charge</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1028</td>\n      <td>1015</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:14:08.845142035Z",
     "start_time": "2024-02-22T19:14:08.376233563Z"
    }
   },
   "id": "a222f783535b926e",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, _df, _tokenizer):\n",
    "        self.labels = _df.columns\n",
    "        self.data = _df.to_dict(orient='records')\n",
    "        self.tokenizer = _tokenizer\n",
    "        x = self.fittest_max_length(_df)\n",
    "        self.max_length = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][self.labels[0]]\n",
    "        y = self.data[idx][self.labels[1]]\n",
    "        text = f\"{x} | {y}\"\n",
    "        tokens = self.tokenizer.encode_plus(text, \n",
    "                                            return_tensors='pt', \n",
    "                                            max_length=1024, \n",
    "                                            truncation=True, \n",
    "                                            padding='max_length')\n",
    "        return tokens\n",
    "    \n",
    "    def fittest_max_length(self, _df):\n",
    "        max_length = max(len(max(_df[self.labels[0]], key=len)), len(max(_df[self.labels[1]], key=len)))\n",
    "        x = 2\n",
    "        while x < max_length: x = x*2\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:14:13.699589346Z",
     "start_time": "2024-02-22T19:14:13.677465160Z"
    }
   },
   "id": "22ef70d26f566522",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data_sample \u001B[38;5;241m=\u001B[39m \u001B[43mLanguageDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[32], line 7\u001B[0m, in \u001B[0;36mLanguageDataset.__init__\u001B[0;34m(self, _df, _tokenizer)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m _df\u001B[38;5;241m.\u001B[39mto_dict(orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m _tokenizer\n\u001B[0;32m----> 7\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfittest_max_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_length \u001B[38;5;241m=\u001B[39m x\n",
      "Cell \u001B[0;32mIn[32], line 25\u001B[0m, in \u001B[0;36mLanguageDataset.fittest_max_length\u001B[0;34m(self, _df)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfittest_max_length\u001B[39m(\u001B[38;5;28mself\u001B[39m, _df):\n\u001B[0;32m---> 25\u001B[0m     max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mmax\u001B[39m(_df[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[\u001B[38;5;241m1\u001B[39m]], key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m)))\n\u001B[1;32m     26\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m x \u001B[38;5;241m<\u001B[39m max_length: x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "data_sample = LanguageDataset(df, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:14:25.914573087Z",
     "start_time": "2024-02-22T19:14:24.433571206Z"
    }
   },
   "id": "5d8d2cf9ef7d7492",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'cell_id_x', 'notebook_id', 'code_imports', 'defined_functions', 'source_x', 'cell_id_y', 'source_y'],\n        num_rows: 828122\n    })\n})"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:14:27.793951248Z",
     "start_time": "2024-02-22T19:14:27.774118988Z"
    }
   },
   "id": "46a42d361847991a",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:449: UserWarning: Length of split at index 0 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(data_sample))\n",
    "val_size = len(data_sample) - train_size\n",
    "\n",
    "train_data, val_data = random_split(data_sample, [train_size, val_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:17:42.385381267Z",
     "start_time": "2024-02-22T19:17:42.328686163Z"
    }
   },
   "id": "d3076057f74959a6",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7a42ee817baabd17"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a09f4ab8fb8d6427"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d9448b7ecddf56dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
